{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO+5CHu1elP/VvDoZV+o27X"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Multi Head attention**"
      ],
      "metadata": {
        "id": "HFwCg9w9q9mo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "cxFhe6XFq6UX"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "\n",
        "  def __init__(self, d_in, d_out, context_len, dropout, num_heads, qkv_bias=False):\n",
        "    super().__init__()\n",
        "    assert (d_out%num_heads==0),\"d_out must be divisible by num_heads\"\n",
        "\n",
        "    self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    self.d_in = d_in\n",
        "    self.d_out = d_out\n",
        "    self.num_heads = num_heads\n",
        "    self.head_dim = d_out//self.num_heads  #divides q*k head into num_head parts\n",
        "    #number of heads we are keeping 2, therefore head_dim is 3\n",
        "\n",
        "    # Here we decided to keep d_in=d_out=6, num_tokens=3\n",
        "\n",
        "    self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "    self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "    self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "    self.output_projection = nn.Linear(d_out, d_out)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    self.register_buffer(\n",
        "        'mask',\n",
        "        torch.triu(torch.ones(context_len, context_len), diagonal=1)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "    b, num_tokens, d_out = x.shape  #(3,3,6)\n",
        "\n",
        "    keys = self.W_key(x)   #shape = (b, num_tokens, d_out)\n",
        "    queries = self.W_query(x)  #b is batch size so if its 3, we will get 3 batches each having tokens in num_tokens qty\n",
        "    values = self.W_value(x)\n",
        "\n",
        "    #now take 1 batch:\n",
        "    # input (1,3,6)---roll out to---->(1,3,2,3)\n",
        "    # (b, num_tokens, d_out) turns into (b, num_tokens, num_heads, head_dim)\n",
        "    # it means we divided one head into 2 heads using view function\n",
        "    keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "    queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "    values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "\n",
        "    #answer ==> keys = torch.tensor([[[0.1,0.2,0.3],  token 1 head 1\n",
        "    #                                [0.4,0.5,0.6]],  token 1 head 2\n",
        "    #                                                                       This is one batch, grouped by tokens\n",
        "    #                                [[0.1,0.2,0.3],  token 2 head 1\n",
        "    #                                [0.4,0.5,0.6]])  token 2 head 2\n",
        "\n",
        "    #                                [[0.1,0.2,0.3],  token 3 head 1\n",
        "    #                                [0.4,0.5,0.6]])  token 3 head 2\n",
        "\n",
        "    #now we have (1,3,2,3) we need to change to (1,2,3,3) so we transpose\n",
        "    #(b, num_tokens, num_heads, head_dim)--->(b, num_heads, num_tokens, head_dim)\n",
        "    keys = keys.transpose(1,2)\n",
        "    queries = queries.transpose(1,2)\n",
        "    values = values.transpose(1,2)\n",
        "\n",
        "    #answer ==> keys = torch.tensor([[[0.1,0.2,0.3],      head 1 token 1\n",
        "    #                                [0.45,0.85, 0.91]    head 1 token 2\n",
        "    #                                [0.32,0.7, 0.5]]]    head 1 token 3\n",
        "    #                                                                       This is one batch, grouped by heads\n",
        "    #                                [[0.1,0.2,0.3],      head 2 token 1\n",
        "    #                                [0.4,0.5,0.6],       head 2 token 2\n",
        "    #                                [0.83,0.98,0.12]]])  head 2 token 3\n",
        "\n",
        "\n",
        "    #only for keys (b, num_heads, num_tokens, head_dim)-------->(b, num_heads, head_dim, num_tokens)\n",
        "    attn_scores = queries @ keys.transpose(2,3)   #transpose to change row to col matrix/ dot product for each head\n",
        "\n",
        "    mask_bool = self.mask.bool()[:num_tokens, :num_tokens]    #causal attention\n",
        "    attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
        "\n",
        "    # scale by dividing it by root of head_dim since it is callign unchanged keys\n",
        "    attn_weights = torch.softmax(attn_scores/keys.shape[-1]**0.5, dim=-1)\n",
        "    attn_weights = self.dropout(attn_weights)\n",
        "\n",
        "    #shape = (b, num_tokens, num_heads, head_dim) grouped by tokens\n",
        "    context_vecs = (attn_weights@values).transpose(1,2)\n",
        "\n",
        "    #now we combine heads where d_out = num_heads*head_dim\n",
        "    context_vecs = context_vecs.contiguous().view(b, num_tokens, self.d_out)  #we roll out back to 3 dims\n",
        "    context_vecs = self.output_projection(context_vecs)\n",
        "\n",
        "    return context_vecs\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "ZUj7lo43rIsv"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.manual_seed(123)\n",
        "\n",
        "inputs = torch.tensor([\n",
        "    [1.0, 2.0, 3.0, 4.0, 5.0, 6.0],\n",
        "    [0.5, 1.5, 2.5, 3.5, 4.5, 5.5],\n",
        "    [9.0, 8.0, 7.0, 6.0, 5.0, 4.0]\n",
        "], dtype=torch.float32)\n",
        "\n",
        "batch = torch.stack((inputs, inputs), dim=0)\n",
        "print(batch.shape)\n",
        "\n",
        "\n",
        "#num_tokens is the context_length\n",
        "batch_size, num_tokens , d_in = batch.shape\n",
        "d_out = batch.shape[-1]\n",
        "\n",
        "mha = MultiHeadAttention(d_in, d_out, num_tokens, 0.0, num_heads=2)\n",
        "context_vecs = mha(batch)\n",
        "print(\"Context vectors: \\n\",context_vecs)\n",
        "print(\"\\nShape of context vectors: \",context_vecs.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "96fsqo4R3Hdg",
        "outputId": "a5a838e4-64ae-48d2-8b7e-48bcf05a9af0"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 3, 6])\n",
            "Context vectors: \n",
            " tensor([[[ 1.4622e-01,  5.7911e-02,  9.8725e-01,  5.6106e-01, -1.0803e+00,\n",
            "          -1.0395e-01],\n",
            "         [ 1.4315e-01, -1.0073e-03,  8.3840e-01,  5.8250e-01, -1.0526e+00,\n",
            "          -9.7606e-02],\n",
            "         [ 1.6335e-01, -3.4609e-03,  8.1940e-01,  5.8568e-01, -1.0263e+00,\n",
            "          -8.2506e-02]],\n",
            "\n",
            "        [[ 1.4622e-01,  5.7911e-02,  9.8725e-01,  5.6106e-01, -1.0803e+00,\n",
            "          -1.0395e-01],\n",
            "         [ 1.4315e-01, -1.0073e-03,  8.3840e-01,  5.8250e-01, -1.0526e+00,\n",
            "          -9.7606e-02],\n",
            "         [ 1.6335e-01, -3.4609e-03,  8.1940e-01,  5.8568e-01, -1.0263e+00,\n",
            "          -8.2506e-02]]], grad_fn=<ViewBackward0>)\n",
            "\n",
            "Shape of context vectors:  torch.Size([2, 3, 6])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "47fcqDQI4qjE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}