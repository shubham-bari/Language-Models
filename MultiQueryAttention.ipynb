{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMuzWdjJ39WaEahQ9De8M1F",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shubham-bari/Language-Models/blob/main/MultiQueryAttention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tensor_size_bytes(t):\n",
        "    return t.nelement() * t.element_size()\n",
        "#function to measure memory usage"
      ],
      "metadata": {
        "id": "cd2aj5Va-atS"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "rILjuRtUxrg1"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from typing import Dict\n",
        "import math\n",
        "\n",
        "class MultiQueryAttention(nn.Module):\n",
        "\n",
        "  def __init__(self, d_in, d_out, context_len, q_heads, dropout=0.0):\n",
        "    super().__init__()\n",
        "    assert d_out%q_heads==0, \"d_out must be divisible by q_heads\"\n",
        "\n",
        "    self.d_in = d_in\n",
        "    self.d_out = d_out\n",
        "    self.context_len = context_len\n",
        "    self.q_heads = q_heads\n",
        "    self.dropout = dropout\n",
        "    self.head_dim = d_out // q_heads\n",
        "\n",
        "    self.Wq = nn.Linear(d_in, d_out, bias=False)\n",
        "    self.Wk = nn.Linear(d_in, d_out, bias=False)\n",
        "    self.Wv = nn.Linear(d_in, d_out, bias=False)\n",
        "\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.out_proj = nn.Linear(d_out, d_out)\n",
        "\n",
        "    self.register_buffer(\n",
        "        'mask',\n",
        "        torch.triu(torch.ones(context_len, context_len), diagonal=1)\n",
        "    )\n",
        "\n",
        "  def forward(self, x, past_kv: Dict[str, torch.tensor]=None):\n",
        "\n",
        "    b, context_len, d_in = x.shape\n",
        "\n",
        "    q = self.Wq(x)\n",
        "    k = self.Wk(x)\n",
        "    v = self.Wv(x)\n",
        "\n",
        "    q = q.view(b, context_len, self.q_heads, self.head_dim)\n",
        "    # shape is (b, 3, 2, 6)\n",
        "\n",
        "    k = k.view(b, context_len, self.q_heads, self.head_dim)\n",
        "    k = k[:, :, :1, :]\n",
        "    #shape is (b,3,1,6)\n",
        "\n",
        "    v = v.view(b, context_len, self.q_heads, self.head_dim)\n",
        "    v = v[:,:, :1, :]\n",
        "\n",
        "\n",
        "    q = q.transpose(1, 2)  #(b,2,3,6)\n",
        "    k = k.transpose(1, 2)  #(b,1,3,6)\n",
        "    v = v.transpose(1, 2)\n",
        "\n",
        "    # If past_kv provided, concatenate along seq_len dim (dim=2)\n",
        "    if past_kv is not None:\n",
        "\n",
        "      # expected shapes for past_kv['k'] and ['v']: (b, num_heads, seq_len_past, head_dim)\n",
        "      past_k = past_kv['k']\n",
        "      past_v = past_kv['v']\n",
        "\n",
        "      if past_k is not None:\n",
        "        k = torch.cat((past_k, k), dim=2)  # new_seq_len = seq_len_past+ seq_len\n",
        "\n",
        "      if past_v is not None:\n",
        "        v = torch.cat((past_v, v), dim=2)\n",
        "\n",
        "    seq_len_total = k.shape[2]\n",
        "\n",
        "    attn_scores = q@k.transpose(2,3)  #k=(b, n_heads, head_dim, s)\n",
        "    attn_scores = attn_scores/math.sqrt(self.head_dim)  #scaling\n",
        "\n",
        "    if past_kv is None:\n",
        "      causal_mask = self.mask.bool()[:context_len, :seq_len_total]\n",
        "      attn_scores.masked_fill_(causal_mask, -torch.inf)\n",
        "    else:\n",
        "      start_idx = seq_len_total - context_len\n",
        "      causal_mask = self.mask.bool()[start_idx:start_idx+context_len, :seq_len_total]\n",
        "      attn_scores.masked_fill_(causal_mask, -torch.inf)\n",
        "\n",
        "    attn_weights = torch.softmax(attn_scores, dim=-1)\n",
        "    attn_weights = self.dropout(attn_weights)\n",
        "\n",
        "    context_vecs = (attn_weights@v).transpose(1,2)\n",
        "\n",
        "    context_vecs = context_vecs.contiguous().view(b, self.context_len , self.d_out)  #we roll out back to 3 dims\n",
        "    context_vecs = self.out_proj(context_vecs)\n",
        "\n",
        "    present_kv=None\n",
        "\n",
        "    present_kv = {'k': k.detach(), 'v': v.detach()}\n",
        "    return context_vecs, present_kv\n",
        "\n",
        "\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import time\n",
        "\n",
        "torch.manual_seed(123)\n",
        "\n",
        "# ---------- Dummy input (same as your MHA test) ----------\n",
        "inputs = torch.tensor([\n",
        "    [1.0, 2.0, 3.0, 4.0, 5.0, 6.0],\n",
        "    [0.5, 1.5, 2.5, 3.5, 4.5, 5.5],\n",
        "    [9.0, 8.0, 7.0, 6.0, 5.0, 4.0]\n",
        "], dtype=torch.float32)\n",
        "\n",
        "batch = torch.stack((inputs, inputs), dim=0)   # shape = (2,3,6)\n",
        "print(\"Batch shape:\", batch.shape)\n",
        "\n",
        "# ---------- Model ----------\n",
        "batch_size, num_tokens, d_in = batch.shape\n",
        "d_out = d_in\n",
        "\n",
        "mqa = MultiQueryAttention(\n",
        "    d_in=d_in,\n",
        "    d_out=d_out,\n",
        "    context_len=num_tokens,\n",
        "    q_heads=2,\n",
        "    dropout=0.0\n",
        ")\n",
        "\n",
        "# ---------- Run MHA-KV without caching ----------\n",
        "start = time.time()\n",
        "context_vecs, present_kv = mqa(batch)\n",
        "end = time.time()\n",
        "\n",
        "print(\"\\nContext vectors:\\n\", context_vecs)\n",
        "print(\"\\nShape:\", context_vecs.shape)\n",
        "print(\"\\nPresent KV shapes:\")\n",
        "print(\"K:\", present_kv['k'].shape)\n",
        "print(\"V:\", present_kv['v'].shape)\n",
        "\n",
        "print(\"\\nTime:\", end - start, \"seconds\")\n",
        "\n",
        "k_mem = tensor_size_bytes(present_kv['k'])\n",
        "v_mem = tensor_size_bytes(present_kv['v'])\n",
        "\n",
        "total = k_mem + v_mem\n",
        "\n",
        "print(\"K memory:\", k_mem, \"bytes\")\n",
        "print(\"V memory:\", v_mem, \"bytes\")\n",
        "print(\"Total KV memory:\", total / 1024, \"KB\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CIiY6syk6Fgm",
        "outputId": "d5f8dd1a-28c0-4b4d-acf7-daeb869a812b"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch shape: torch.Size([2, 3, 6])\n",
            "\n",
            "Context vectors:\n",
            " tensor([[[ 0.8755,  0.1901,  1.4868, -0.5051, -0.0724,  0.6745],\n",
            "         [ 0.8412,  0.1574,  1.3448, -0.4141, -0.1003,  0.6521],\n",
            "         [ 2.1408, -0.7935,  3.4938, -2.1454,  1.0740,  0.0329]],\n",
            "\n",
            "        [[ 0.8755,  0.1901,  1.4868, -0.5051, -0.0724,  0.6745],\n",
            "         [ 0.8412,  0.1574,  1.3448, -0.4141, -0.1003,  0.6521],\n",
            "         [ 2.1408, -0.7935,  3.4938, -2.1454,  1.0740,  0.0329]]],\n",
            "       grad_fn=<ViewBackward0>)\n",
            "\n",
            "Shape: torch.Size([2, 3, 6])\n",
            "\n",
            "Present KV shapes:\n",
            "K: torch.Size([2, 1, 3, 3])\n",
            "V: torch.Size([2, 1, 3, 3])\n",
            "\n",
            "Time: 0.0015461444854736328 seconds\n",
            "K memory: 72 bytes\n",
            "V memory: 72 bytes\n",
            "Total KV memory: 0.140625 KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see from above that the memory used by KV cache has almost reduced by 50% or a factor of 1/2 (n_heads=2) than normally\n",
        "\n"
      ],
      "metadata": {
        "id": "XbrtVLLx-xXG"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9HRaEmyS6ZcO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}