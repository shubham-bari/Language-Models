{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMmngIDq0zMP3N1hpy2+pb6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shubham-bari/Language-Models/blob/main/MHA_KV_Cache.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w_uYy7x9Ln0i"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "from typing import Dict"
      ],
      "metadata": {
        "id": "wQk0ZTO6SYIo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tensor_size_bytes(t):\n",
        "    return t.nelement() * t.element_size()\n",
        "#function to measure memory usage"
      ],
      "metadata": {
        "id": "p9HHyjSv-HCM"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadKV(nn.Module):\n",
        "\n",
        "  def __init__(self, d_in, d_out, context_len, n_heads, dropout: float=0.0):\n",
        "    super().__init__()\n",
        "    assert d_out%n_heads==0, \"d_out must be divisible by n_heads\"\n",
        "\n",
        "    self.d_in = d_in\n",
        "    self.d_out = d_out\n",
        "    self.n_heads = n_heads\n",
        "    self.head_dim = d_out//n_heads\n",
        "    self.context_len = context_len\n",
        "    self.Wq = nn.Linear(d_in, d_out, bias=False)\n",
        "    self.Wk = nn.Linear(d_in, d_out, bias=False)\n",
        "    self.Wv = nn.Linear(d_in, d_out, bias=False)\n",
        "\n",
        "    self.output_projection = nn.Linear(d_out, d_out, bias = False)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    self.register_buffer(\n",
        "        'mask',\n",
        "        torch.triu(torch.ones(context_len, context_len), diagonal=1)\n",
        "    )\n",
        "\n",
        "  def forward(self, x, past_kv: Dict[str, torch.tensor]=None):\n",
        "\n",
        "    b, context_size, d_out = x.shape\n",
        "    batch_size = b\n",
        "\n",
        "    q = self.Wq(x)\n",
        "    k = self.Wk(x)\n",
        "    v = self.Wv(x)\n",
        "\n",
        "    q = q.view(batch_size, context_size, self.n_heads, self.head_dim)\n",
        "    k = k.view(batch_size, context_size, self.n_heads, self.head_dim)\n",
        "    v = v.view(batch_size, context_size, self.n_heads, self.head_dim)\n",
        "\n",
        "    q = q.transpose(1, 2)\n",
        "    k = k.transpose(1, 2)\n",
        "    v = v.transpose(1, 2)\n",
        "\n",
        "    # If past_kv provided, concatenate along seq_len dim (dim=2)\n",
        "    if past_kv is not None:\n",
        "\n",
        "      # expected shapes for past_kv['k'] and ['v']: (b, num_heads, seq_len_past, head_dim)\n",
        "      past_k = past_kv['k']\n",
        "      past_v = past_kv['v']\n",
        "\n",
        "      if past_k is not None:\n",
        "        k = torch.cat((past_k, k), dim=2)  # new_seq_len = seq_len_past+ seq_len\n",
        "\n",
        "      if past_v is not None:\n",
        "        v = torch.cat((past_v, v), dim=2)\n",
        "\n",
        "    seq_len_total = k.shape[2]\n",
        "\n",
        "    attn_scores = torch.matmul(q, k.transpose(2,3))  #k=(b, n_heads, head_dim, s)\n",
        "    attn_scores = attn_scores/math.sqrt(self.head_dim)  #scaling\n",
        "\n",
        "    if past_kv is None:\n",
        "      causal_mask = self.mask.bool()[:context_size, :seq_len_total]\n",
        "      attn_scores.masked_fill_(causal_mask, -torch.inf)\n",
        "    else:\n",
        "      start_idx = seq_len_total - context_size\n",
        "      causal_mask = self.mask.bool()[start_idx:start_idx+context_size, :seq_len_total]\n",
        "      attn_scores.masked_fill_(causal_mask, -torch.inf)\n",
        "\n",
        "    attn_weights = torch.softmax(attn_scores, dim=-1)\n",
        "    attn_weights = self.dropout(attn_weights)\n",
        "\n",
        "    context_vecs = (attn_weights@v).transpose(1,2)\n",
        "\n",
        "    context_vecs = context_vecs.contiguous().view(b, self.context_len , self.d_out)  #we roll out back to 3 dims\n",
        "    context_vecs = self.output_projection(context_vecs)\n",
        "\n",
        "    present_kv=None\n",
        "\n",
        "    present_kv = {'k': k.detach(), 'v': v.detach()}\n",
        "    return context_vecs, present_kv\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NPCLSuyFSZ4m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import time\n",
        "\n",
        "torch.manual_seed(123)\n",
        "\n",
        "# ---------- Dummy input (same as your MHA test) ----------\n",
        "inputs = torch.tensor([\n",
        "    [1.0, 2.0, 3.0, 4.0, 5.0, 6.0],\n",
        "    [0.5, 1.5, 2.5, 3.5, 4.5, 5.5],\n",
        "    [9.0, 8.0, 7.0, 6.0, 5.0, 4.0]\n",
        "], dtype=torch.float32)\n",
        "\n",
        "batch = torch.stack((inputs, inputs), dim=0)   # shape = (2,3,6)\n",
        "print(\"Batch shape:\", batch.shape)\n",
        "\n",
        "# ---------- Model ----------\n",
        "batch_size, num_tokens, d_in = batch.shape\n",
        "d_out = d_in\n",
        "\n",
        "mha_kv = MultiHeadKV(\n",
        "    d_in=d_in,\n",
        "    d_out=d_out,\n",
        "    context_len=num_tokens,\n",
        "    n_heads=2,\n",
        "    dropout=0.0\n",
        ")\n",
        "\n",
        "# ---------- Run MHA-KV without caching ----------\n",
        "start = time.time()\n",
        "context_vecs, present_kv = mha_kv(batch)\n",
        "end = time.time()\n",
        "\n",
        "print(\"\\nContext vectors:\\n\", context_vecs)\n",
        "print(\"\\nShape:\", context_vecs.shape)\n",
        "print(\"\\nPresent KV shapes:\")\n",
        "print(\"K:\", present_kv['k'].shape)\n",
        "print(\"V:\", present_kv['v'].shape)\n",
        "\n",
        "print(\"\\nTime:\", end - start, \"seconds\")\n",
        "\n",
        "k_mem = tensor_size_bytes(present_kv['k'])\n",
        "v_mem = tensor_size_bytes(present_kv['v'])\n",
        "\n",
        "total = k_mem + v_mem\n",
        "\n",
        "print(\"K memory:\", k_mem, \"bytes\")\n",
        "print(\"V memory:\", v_mem, \"bytes\")\n",
        "print(\"Total KV memory:\", total / 1024, \"KB\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJquCuuxzkPx",
        "outputId": "48e10bee-e860-40be-e095-e04600d8d641"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch shape: torch.Size([2, 3, 6])\n",
            "\n",
            "Context vectors:\n",
            " tensor([[[ 0.0844,  0.2387,  1.2366,  0.5299, -0.7774,  0.2029],\n",
            "         [ 0.0813,  0.1798,  1.0878,  0.5513, -0.7497,  0.2093],\n",
            "         [ 0.1015,  0.1774,  1.0688,  0.5545, -0.7234,  0.2244]],\n",
            "\n",
            "        [[ 0.0844,  0.2387,  1.2366,  0.5299, -0.7774,  0.2029],\n",
            "         [ 0.0813,  0.1798,  1.0878,  0.5513, -0.7497,  0.2093],\n",
            "         [ 0.1015,  0.1774,  1.0688,  0.5545, -0.7234,  0.2244]]],\n",
            "       grad_fn=<UnsafeViewBackward0>)\n",
            "\n",
            "Shape: torch.Size([2, 3, 6])\n",
            "\n",
            "Present KV shapes:\n",
            "K: torch.Size([2, 2, 3, 3])\n",
            "V: torch.Size([2, 2, 3, 3])\n",
            "\n",
            "Time: 0.0009682178497314453 seconds\n",
            "K memory: 144 bytes\n",
            "V memory: 144 bytes\n",
            "Total KV memory: 0.28125 KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that the KV cache takes 144 bytes of memory which is directly proportional to the number of attention heads used\n"
      ],
      "metadata": {
        "id": "Mz6Fblz3_Gyq"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vofkC17r_GZJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "E0VYqCKZ_GWu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "B5fO-yGVYRUc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}